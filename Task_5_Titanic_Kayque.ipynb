{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TASK 5 - TITANIC!.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lnRNZhWhZMg"
      },
      "source": [
        "###TASK 5: TITANIC KAGGLE CHALLENGE\n",
        "The following code is an attempt to try to answer the kaggle challenge from Titanic dataset on: https://www.kaggle.com/c/titanic/overview/evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIMejemzh5v4"
      },
      "source": [
        "FIRST, LETS IMPORT WGET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8BA1_Cchajp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "14043210-5bf2-4029-b768-a64bcc3cbe4a"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3b5-cW2iBVr"
      },
      "source": [
        "NOW, LET'S USE IT TO DOWNLOAD THE DATASET!\n",
        "\n",
        "Unfortunately, after a few days of struggle, I couldnt find a way to solely download the dataset from kaggle. So please download it from the webpage and import it to your google drive, doin the necessary changes on directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWfbPAGPjiiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "842d8163-2e90-45b2-a717-aae870dbaf08"
      },
      "source": [
        "import wget, os, zipfile\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "#!ls \"/content/drive/My Drive/CURSO_Redes_e_IA_Clécio/TITANIC_CHALLENGE/\"\n",
        "if os.path.exists('/content/drive/My Drive/CURSO_Redes_e_IA_Clécio/TITANIC_CHALLENGE/gender_submission.csv'):\n",
        "    print(' ** Dataset already downloaded!')\n",
        "    print(' ** Your dataset is ready to rock.')\n",
        "else:\n",
        "    print(' ** Dataset not found. Please download it and import into your drive.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            " ** Dataset already downloaded!\n",
            " ** Your dataset is ready to rock.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oVFZnytBogq"
      },
      "source": [
        "NOW IT'S TIME TO DO PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eshWFUI4PSDH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "7d6040bb-0e19-41a5-abc3-2e307991d92d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "gender = pd.read_csv('/content/drive/My Drive/CURSO_Redes_e_IA_Clécio/TITANIC_CHALLENGE/gender_submission.csv', header=0)\n",
        "test = pd.read_csv('/content/drive/My Drive/CURSO_Redes_e_IA_Clécio/TITANIC_CHALLENGE/test.csv', header=0)\n",
        "train = pd.read_csv('/content/drive/My Drive/CURSO_Redes_e_IA_Clécio/TITANIC_CHALLENGE/train.csv', header=0)\n",
        "\n",
        "print(' ** Training set shape is:', train.shape,\n",
        "      '\\n ** It has the following data types:', train.columns,\n",
        "      '\\n ** Test set is:', test.shape,\n",
        "      '\\n ** Gender set is:', gender.shape,\n",
        "      '\\n ** This dataset has the following columns:', train.columns,\n",
        "      \"\\n ** You may notice they're full of problems. Let's check them out.\")\n",
        "\n",
        "print(\"\\n ** First, the labels column is inside the train data.\\\n",
        "Let's separate training data from labels.\")\n",
        "labels = train['Survived']\n",
        "print(\" ** We'll be also saving the victim's names.\")\n",
        "train_victim_names = train['Name']\n",
        "test_victim_names = test['Name']\n",
        "del train['Survived'] \n",
        "print(' ** Training set new shape is:', train.shape)\n",
        "\n",
        "print(\" ** Let's check for missing data...\")\n",
        "print(\" ** Well, there is some missing data on Age column. Let's replace it \\\n",
        "with median. \\n ** There's on 'Embarked' column as well, so we're gonna replace\\\n",
        "for 'S', as it's the string with most ocurrencies.\")\n",
        "\n",
        "print(\" ** Ticket, Name and Cabin column doesn't seem very helpful. Let's remove it.\")\n",
        "del train['Ticket'], test['Ticket'], train['Name'], test['Name'], \\\n",
        "    train['Cabin'], test['Cabin'], train['PassengerId'], test['PassengerId']\n",
        "\n",
        "medians = train['Age'].median()\n",
        "mediansf = train['Fare'].median()\n",
        "\n",
        "train['Age'] = train['Age'].replace(to_replace=np.NaN,value=medians)\n",
        "test['Age'] = test['Age'].replace(to_replace=np.NaN,value=medians)\n",
        "train['Fare'] = train['Fare'].replace(to_replace=np.NaN,value=mediansf)\n",
        "test['Fare'] = test['Fare'].replace(to_replace=np.NaN,value=mediansf)\n",
        "train['Embarked'] = train['Embarked'].replace(to_replace=np.NaN,value='S')\n",
        "test['Embarked'] = test['Embarked'].replace(to_replace=np.NaN,value='S')\n",
        "from collections import Counter\n",
        "cn = Counter(train['Embarked'])\n",
        "###FIZEMOS ISSO PORQUE NÃO HÁ GARANTIA DE QUE NO GRUPO DE TESTE NÃO SERIAM\n",
        "##NECESSÁRIAS INCLUIR NOVAS CLASSES, QUE NUNCA APARECERAM ANTES.\n",
        "print(\" ** Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ** Training set shape is: (891, 12) \n",
            " ** It has the following data types: Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
            "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
            "      dtype='object') \n",
            " ** Test set is: (418, 11) \n",
            " ** Gender set is: (418, 2) \n",
            " ** This dataset has the following columns: Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
            "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
            "      dtype='object') \n",
            " ** You may notice they're full of problems. Let's check them out.\n",
            "\n",
            " ** First, the labels column is inside the train data.Let's separate training data from labels.\n",
            " ** We'll be also saving the victim's names.\n",
            " ** Training set new shape is: (891, 11)\n",
            " ** Let's check for missing data...\n",
            " ** Well, there is some missing data on Age column. Let's replace it with median. \n",
            " ** There's on 'Embarked' column as well, so we're gonna replacefor 'S', as it's the string with most ocurrencies.\n",
            " ** Ticket, Name and Cabin column doesn't seem very helpful. Let's remove it.\n",
            " ** Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16fd4SOm59F7"
      },
      "source": [
        "CONTINUE PREPROCESSING, WITH FINAL STEPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9RZiJgq6B7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "af874e98-a238-4d0c-a7c9-22b762a7b3fd"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer \n",
        "\n",
        "print(\" ** It's time to convert these strings into values.\")\n",
        "LE = LabelEncoder()\n",
        "allcolumns = train.columns\n",
        "columns = ['Sex', 'Embarked']\n",
        "\n",
        "#for i in columns:\n",
        "train['Sex'] = LE.fit_transform(train['Sex'])\n",
        "test['Sex'] = LE.transform(test['Sex'])\n",
        "train['Embarked'] = LE.fit_transform(train['Embarked'])\n",
        "test['Embarked'] = LE.transform(test['Embarked'])\n",
        "\n",
        "\n",
        "print(\" ** Next action: Let's normalyze those values!\")\n",
        "\n",
        "SS = StandardScaler()\n",
        "train = SS.fit_transform(train)\n",
        "test = SS.transform(test)\n",
        "\n",
        "num_classes = train.shape[1]\n",
        "print(\" ** Here we have a training set with %s classes.\" % num_classes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ** It's time to convert these strings into values.\n",
            " ** Next action: Let's normalyze those values!\n",
            " ** Here we have a training set with 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvntUZVh3mSK"
      },
      "source": [
        "NOW, LET'S TRAIN SOME CLASSIFIERS AND EVALUATE THEM! WHICH OF THESE SHOULD DO BEST?  \n",
        "Feel free to check their correlation graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNJwOAki3pOf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "744c3f72-986b-4994-fe8f-ba1f887a265d"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_1 = RandomForestClassifier()\n",
        "model_1.fit(train, labels)\n",
        "survivors = model_1.predict(test)\n",
        "acc = round(model_1.score(train, labels)*100, 2)\n",
        "print(' ** Accuracy from Random Forest Classifier: {} %'. format(acc))\n",
        "\n",
        "from sklearn import svm\n",
        "model_2 = svm.SVC()\n",
        "model_2.fit(train, labels)\n",
        "acc = round(model_2.score(train, labels)*100, 2)\n",
        "print(' ** Accuracy from Support Vector Machine Classifier: {} %'. format(acc))\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model_3 = GaussianNB()\n",
        "model_3.fit(train, labels)\n",
        "acc = round(model_3.score(train, labels)*100, 2)\n",
        "print(' ** Accuracy from Gaussian Naive-Bayes: {} %'. format(acc))\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model_4 = KNeighborsClassifier(n_neighbors = 3)\n",
        "model_4.fit(train, labels)\n",
        "acc = round(model_4.score(train, labels)*100, 2)\n",
        "print(' ** Accuracy from K Neighbours Classifier: {} %'. format(acc))\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model_5 = DecisionTreeClassifier()\n",
        "model_5.fit(train, labels)\n",
        "acc = round(model_5.score(train, labels)*100, 2)\n",
        "print(' ** Accuracy from Decision Tree Classifier: {} %'. format(acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ** Accuracy from Random Forest Classifier: 97.87 %\n",
            " ** Accuracy from Support Vector Machine Classifier: 83.95 %\n",
            " ** Accuracy from Gaussian Naive-Bayes: 79.24 %\n",
            " ** Accuracy from K Neighbours Classifier: 87.43 %\n",
            " ** Accuracy from Decision Tree Classifier: 97.98 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWnDzNAGUMO3"
      },
      "source": [
        "Let's use the best classifier to now check out who survived.  \n",
        "One minute of silence now, as we check the \n",
        "obituary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ6zxAuvUn9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "outputId": "268f8054-1e22-45bf-d8bc-1fa94933995e"
      },
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "survived = np.empty(shape=(0,0))\n",
        "for x in range(len(survivors)):\n",
        "    if survivors[x] == 0:\n",
        "        result = 'No'\n",
        "    else:\n",
        "        result = 'Yes'\n",
        "    survived = np.append(survived, result)\n",
        "print(survived.shape)\n",
        "    \n",
        "table = [(test_victim_names, survived)]\n",
        "print(tabulate(table, headers=[\"Name\", \"Survived?\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(418,)\n",
            "Name                                                 Survived?\n",
            "---------------------------------------------------  --------------------------------------------------------------------------\n",
            "0                                  Kelly, Mr. James  ['No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'Yes' 'No'\n",
            "1                  Wilkes, Mrs. James (Ellen Needs)   'Yes' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'Yes' 'Yes' 'Yes' 'No' 'Yes'\n",
            "2                         Myles, Mr. Thomas Francis   'Yes' 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'No'\n",
            "3                                  Wirz, Mr. Albert   'No' 'Yes' 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'Yes'\n",
            "4      Hirvonen, Mrs. Alexander (Helga E Lindqvist)   'Yes' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'Yes' 'Yes' 'Yes'\n",
            "                           ...                        'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No'\n",
            "413                              Spector, Mr. Woolf   'Yes' 'Yes' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'Yes' 'Yes' 'No' 'No'\n",
            "414                    Oliva y Ocana, Dona. Fermina   'Yes' 'No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'No' 'No'\n",
            "415                    Saether, Mr. Simon Sivertsen   'No' 'No' 'No' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'No'\n",
            "416                             Ware, Mr. Frederick   'Yes' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'No'\n",
            "417                        Peter, Master. Michael J   'No' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No'\n",
            "Name: Name, Length: 418, dtype: object                'No' 'No' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'Yes'\n",
            "                                                      'Yes' 'Yes' 'Yes' 'Yes' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No' 'No'\n",
            "                                                      'No' 'No' 'No' 'Yes' 'Yes' 'Yes' 'Yes' 'Yes' 'No' 'No' 'Yes' 'No' 'Yes'\n",
            "                                                      'No' 'Yes' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'Yes' 'No' 'Yes' 'No' 'No'\n",
            "                                                      'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'No'\n",
            "                                                      'No' 'No' 'No' 'Yes' 'No' 'Yes' 'No' 'Yes' 'No' 'Yes' 'Yes' 'Yes' 'No'\n",
            "                                                      'No' 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'Yes' 'Yes' 'Yes'\n",
            "                                                      'Yes' 'No' 'Yes' 'No' 'No' 'No' 'No' 'Yes' 'No' 'Yes' 'No' 'Yes' 'No'\n",
            "                                                      'Yes' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'Yes' 'Yes' 'No' 'No'\n",
            "                                                      'No' 'No' 'No' 'No' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'No' 'No' 'No'\n",
            "                                                      'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'No'\n",
            "                                                      'No' 'No' 'Yes' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'Yes' 'Yes' 'Yes'\n",
            "                                                      'Yes' 'No' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'No' 'No'\n",
            "                                                      'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'No' 'No' 'Yes' 'Yes' 'No' 'No'\n",
            "                                                      'No' 'No' 'Yes' 'No' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'Yes' 'No' 'No'\n",
            "                                                      'No' 'Yes' 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'No'\n",
            "                                                      'Yes' 'Yes' 'Yes' 'Yes' 'No' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'No'\n",
            "                                                      'Yes' 'Yes' 'No' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'No'\n",
            "                                                      'No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'No'\n",
            "                                                      'Yes' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'Yes' 'Yes' 'No' 'No' 'Yes'\n",
            "                                                      'No' 'No' 'Yes']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
